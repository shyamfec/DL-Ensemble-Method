{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IVP_DataGen_Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vnRIcXez9Fe"
      },
      "source": [
        "!pip install spectral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua5OYF4Fv97s"
      },
      "source": [
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = str(0)\n",
        "\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "import keras\n",
        "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, AveragePooling3D, concatenate\n",
        "from keras.layers import Dropout, Input, MaxPooling3D, MaxPooling2D, AveragePooling2D, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "from keras.backend import backend\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "\n",
        "from operator import truediv\n",
        "\n",
        "from plotly.offline import init_notebook_mode\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import spectral\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXJGT4yQhyoM"
      },
      "source": [
        "import tensorflow.keras\n",
        "# from keras.utils import Sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFhTC0zOz33J"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6SpnXpkv97t"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_AmPBBlv97t"
      },
      "source": [
        "## GLOBAL VARIABLES\n",
        "dataset = 'SA'\n",
        "test_ratio = 0.8\n",
        "windowSize = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcW8xKahv97t"
      },
      "source": [
        "def loadData(name):\n",
        "    data_path = os.path.join(os.getcwd(),'/content/drive/MyDrive/Dataset_IVP')\n",
        "    if name == 'IP':\n",
        "        data = sio.loadmat(os.path.join(data_path, 'Indian_pines_corrected.mat'))['indian_pines_corrected']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'))['indian_pines_gt']\n",
        "    elif name == 'SA':\n",
        "        data = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
        "    elif name == 'PU':\n",
        "        data = sio.loadmat(os.path.join(data_path, 'PaviaU.mat'))['paviaU']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'PaviaU_gt.mat'))['paviaU_gt']\n",
        "    \n",
        "    return data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucx1lLa9v97t"
      },
      "source": [
        "def splitTrainTestSet(X, y, testRatio):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=0,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEmwkJtlv97t"
      },
      "source": [
        "def applyPCA(X, numComponents=75):\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
        "    return newX, pca"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-O1SWFxv97u"
      },
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rll80tZhv97u"
      },
      "source": [
        "# One time run if saving in drive\n",
        "def createImageCubes(X, y, windowSize=25):\n",
        "    margin = int((windowSize - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    indices = []\n",
        "    labels = []\n",
        "    index = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            curr_label = y[r-margin, c-margin]  \n",
        "\n",
        "            if curr_label>0:\n",
        "              np.save('data_'+str(index)+'.npy',patch.reshape(windowSize, windowSize, X.shape[2], 1))\n",
        "              indices.append(index)\n",
        "              labels.append(curr_label)\n",
        "              index+=1\n",
        "            else:\n",
        "              index+=1\n",
        "\n",
        "    return indices, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1os1SGVUtAul"
      },
      "source": [
        "class CubeDataGenerator(tensorflow.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dim=(25,25,15), n_channels=1, n_classes=16, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            X[i,] = np.load('data_' + str(ID) + '.npy')[:,:,:,0]\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID] - 1\n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DujKUkBgv97u"
      },
      "source": [
        "X, y = loadData(dataset)\n",
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-tBLe6sv97u"
      },
      "source": [
        "K = 30 if dataset == 'IP' else 15\n",
        "X,pca = applyPCA(X,numComponents=K)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prwd9sNAxdso"
      },
      "source": [
        "list_indices, list_labels = createImageCubes(X, y, windowSize=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuMNN6pCv97v"
      },
      "source": [
        "train_indices, test_indices, train_labels, test_labels = splitTrainTestSet(list_indices, list_labels, test_ratio)\n",
        "\n",
        "len(train_indices), len(test_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTJaci2Kv97v"
      },
      "source": [
        "# list_IDs, labels, batch_size=32, dim=(25,25,15), n_channels=1, n_classes=16\n",
        "output_units = 9 if (dataset == 'PU' or dataset == 'PC') else 16\n",
        "new_y = y.flatten()\n",
        "# TrainGen = CubeDataGenerator(train_indices, new_y, 32, (25,25,15), 1, output_units)\n",
        "# TestGen = CubeDataGenerator(test_indices, new_y, 1, (25,25,15), 1, output_units, shuffle=False)\n",
        "TrainGen = CubeDataGenerator(train_indices, new_y, 32, (25,25), K, output_units)\n",
        "TestGen = CubeDataGenerator(test_indices, new_y, 1, (25,25), K, output_units, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN9wz07Yv97v"
      },
      "source": [
        "# Model and Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ciOUdT7v97v"
      },
      "source": [
        "S = windowSize\n",
        "L = K\n",
        "output_units = 9 if (dataset == 'PU' or dataset == 'PC') else 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL4xw7E-XqL-"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization, Conv3D, MaxPooling3D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Concatenate, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers, activations\n",
        "import os\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "#import itertools\n",
        "#import shutil\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ7BB8hPv97w"
      },
      "source": [
        "def conv2d(x,numfilt,filtsz,strides=1,pad='same',act=True,name=None):\n",
        "  x = Conv2D(numfilt,filtsz,strides,padding=pad,data_format='channels_last',use_bias=False,name=name+'conv2d')(x)\n",
        "  x = BatchNormalization(axis=3,scale=False,name=name+'conv2d'+'bn')(x)\n",
        "  if act:\n",
        "    x = Activation('relu',name=name+'conv2d'+'act')(x)\n",
        "  return x\n",
        "\n",
        "def incresA(x,scale,name=None):\n",
        "    pad = 'same'\n",
        "    branch0 = conv2d(x,32,1,1,pad,True,name=name+'b0')\n",
        "    branch1 = conv2d(x,32,1,1,pad,True,name=name+'b1_1')\n",
        "    branch1 = conv2d(branch1,32,3,1,pad,True,name=name+'b1_2')\n",
        "    branch2 = conv2d(x,32,1,1,pad,True,name=name+'b2_1')\n",
        "    branch2 = conv2d(branch2,48,3,1,pad,True,name=name+'b2_2')\n",
        "    branch2 = conv2d(branch2,64,3,1,pad,True,name=name+'b2_3')\n",
        "    branches = [branch0,branch1,branch2]\n",
        "    mixed = Concatenate(axis=3, name=name + '_concat')(branches)\n",
        "    filt_exp_1x1 = conv2d(mixed,384,1,1,pad,False,name=name+'filt_exp_1x1')\n",
        "    final_lay = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n",
        "                      output_shape=backend.int_shape(x)[1:],\n",
        "                      arguments={'scale': scale},\n",
        "                      name=name+'act_scaling')([x, filt_exp_1x1])\n",
        "    return final_lay\n",
        "\n",
        "def incresB(x,scale,name=None):\n",
        "    pad = 'same'\n",
        "    branch0 = conv2d(x,192,1,1,pad,True,name=name+'b0')\n",
        "    branch1 = conv2d(x,128,1,1,pad,True,name=name+'b1_1')\n",
        "    branch1 = conv2d(branch1,160,[1,7],1,pad,True,name=name+'b1_2')\n",
        "    branch1 = conv2d(branch1,192,[7,1],1,pad,True,name=name+'b1_3')\n",
        "    branches = [branch0,branch1]\n",
        "    mixed = Concatenate(axis=3, name=name + '_mixed')(branches)\n",
        "    filt_exp_1x1 = conv2d(mixed,1152,1,1,pad,False,name=name+'filt_exp_1x1')\n",
        "    final_lay = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n",
        "                      output_shape=backend.int_shape(x)[1:],\n",
        "                      arguments={'scale': scale},\n",
        "                      name=name+'act_scaling')([x, filt_exp_1x1])\n",
        "    return final_lay\n",
        "\n",
        "def incresC(x,scale,name=None):\n",
        "    pad = 'same'\n",
        "    branch0 = conv2d(x,192,1,1,pad,True,name=name+'b0')\n",
        "    branch1 = conv2d(x,192,1,1,pad,True,name=name+'b1_1')\n",
        "    branch1 = conv2d(branch1,224,[1,3],1,pad,True,name=name+'b1_2')\n",
        "    branch1 = conv2d(branch1,256,[3,1],1,pad,True,name=name+'b1_3')\n",
        "    branches = [branch0,branch1]\n",
        "    mixed = Concatenate(axis=3, name=name + '_mixed')(branches)\n",
        "    filt_exp_1x1 = conv2d(mixed,2048,1,1,pad,False,name=name+'fin1x1')\n",
        "    final_lay = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n",
        "                      output_shape=backend.int_shape(x)[1:],\n",
        "                      arguments={'scale': scale},\n",
        "                      name=name+'act_saling')([x, filt_exp_1x1])\n",
        "    return final_lay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZq-6Q6MGMC7"
      },
      "source": [
        "img_input = Input(shape=(25,25,K))\n",
        "# img_input = Input(shape=(25,25,30))\n",
        "\n",
        "x = conv2d(img_input,32,3,2,'same',True,name='conv1')\n",
        "x = conv2d(x,32,3,1,'same',True,name='conv2')\n",
        "x = conv2d(x,64,3,1,'same',True,name='conv3')\n",
        "\n",
        "x_11 = MaxPooling2D(3,strides=1,padding='same',name='stem_br_11'+'_maxpool_1')(x)\n",
        "x_12 = conv2d(x,64,3,1,'same',True,name='stem_br_12')\n",
        "\n",
        "x = Concatenate(axis=3, name = 'stem_concat_1')([x_11,x_12])\n",
        "\n",
        "x_21 = conv2d(x,64,1,1,'same',True,name='stem_br_211')\n",
        "x_21 = conv2d(x_21,64,[1,7],1,'same',True,name='stem_br_212')\n",
        "x_21 = conv2d(x_21,64,[7,1],1,'same',True,name='stem_br_213')\n",
        "x_21 = conv2d(x_21,96,3,1,'same',True,name='stem_br_214')\n",
        "\n",
        "x_22 = conv2d(x,64,1,1,'same',True,name='stem_br_221')\n",
        "x_22 = conv2d(x_22,96,3,1,'same',True,name='stem_br_222')\n",
        "\n",
        "x = Concatenate(axis=3, name = 'stem_concat_2')([x_21,x_22])\n",
        "\n",
        "x_31 = conv2d(x,192,3,1,'same',True,name='stem_br_31')\n",
        "x_32 = MaxPooling2D(3,strides=1,padding='same',name='stem_br_32'+'_maxpool_2')(x)\n",
        "x = Concatenate(axis=3, name = 'stem_concat_3')([x_31,x_32])\n",
        "\n",
        "#Inception-ResNet-A modules\n",
        "x = incresA(x,0.15,name='incresA_1')\n",
        "x = incresA(x,0.15,name='incresA_2')\n",
        "x = incresA(x,0.15,name='incresA_3')\n",
        "x = incresA(x,0.15,name='incresA_4')\n",
        "\n",
        "#35 × 35 to 17 × 17 reduction module.\n",
        "x_red_11 = MaxPooling2D(3,strides=2,padding='valid',name='red_maxpool_1')(x)\n",
        "\n",
        "x_red_12 = conv2d(x,384,3,2,'valid',True,name='x_red1_c1')\n",
        "\n",
        "x_red_13 = conv2d(x,256,1,1,'same',True,name='x_red1_c2_1')\n",
        "x_red_13 = conv2d(x_red_13,256,3,1,'same',True,name='x_red1_c2_2')\n",
        "x_red_13 = conv2d(x_red_13,384,3,2,'valid',True,name='x_red1_c2_3')\n",
        "\n",
        "x = Concatenate(axis=3, name='red_concat_1')([x_red_11,x_red_12,x_red_13])\n",
        "\n",
        "#Inception-ResNet-B modules\n",
        "x = incresB(x,0.1,name='incresB_1')\n",
        "x = incresB(x,0.1,name='incresB_2')\n",
        "x = incresB(x,0.1,name='incresB_3')\n",
        "x = incresB(x,0.1,name='incresB_4')\n",
        "x = incresB(x,0.1,name='incresB_5')\n",
        "x = incresB(x,0.1,name='incresB_6')\n",
        "x = incresB(x,0.1,name='incresB_7')\n",
        "\n",
        "#17 × 17 to 8 × 8 reduction module.\n",
        "x_red_21 = MaxPooling2D(3,strides=2,padding='valid',name='red_maxpool_2')(x)\n",
        "\n",
        "x_red_22 = conv2d(x,256,1,1,'same',True,name='x_red2_c11')\n",
        "x_red_22 = conv2d(x_red_22,384,3,2,'valid',True,name='x_red2_c12')\n",
        "\n",
        "x_red_23 = conv2d(x,256,1,1,'same',True,name='x_red2_c21')\n",
        "x_red_23 = conv2d(x_red_23,256,3,2,'valid',True,name='x_red2_c22')\n",
        "\n",
        "x_red_24 = conv2d(x,256,1,1,'same',True,name='x_red2_c31')\n",
        "x_red_24 = conv2d(x_red_24,256,3,1,'same',True,name='x_red2_c32')\n",
        "x_red_24 = conv2d(x_red_24,256,3,2,'valid',True,name='x_red2_c33')\n",
        "\n",
        "x = Concatenate(axis=3, name='red_concat_2')([x_red_21,x_red_22,x_red_23,x_red_24])\n",
        "\n",
        "#Inception-ResNet-C modules\n",
        "x = incresC(x,0.2,name='incresC_1')\n",
        "x = incresC(x,0.2,name='incresC_2')\n",
        "x = incresC(x,0.2,name='incresC_3')\n",
        "\n",
        "#TOP\n",
        "x = GlobalAveragePooling2D(data_format='channels_last')(x)\n",
        "x = Dropout(0.6)(x)\n",
        "x = Dense(output_units, activation='softmax')(x)\n",
        "# x = Dense(16, activation='softmax')(x)\n",
        "\n",
        "model_IR = Model(img_input,x,name=\"inception_resnet_v2\")\n",
        "model_IR.summary()\n",
        "\n",
        "# plot_model(model, show_shapes=False, to_file='inception_resnet_module.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHfb8snYv97w"
      },
      "source": [
        "# compiling the model\n",
        "adam = Adam(lr=0.001, decay=1e-06)\n",
        "model_IR.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TUv8q_Mv97w"
      },
      "source": [
        "# checkpoint\n",
        "filepath = \"best_model_IR.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwu6JPkU2FAj"
      },
      "source": [
        "model_IR.fit_generator(TrainGen, steps_per_epoch=len(train_indices)//32, epochs=75, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqMKM0VNZvg_"
      },
      "source": [
        "input_layer1 = Input((S, S, L, 1))\n",
        "\n",
        "## convolutional layers\n",
        "conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 7), activation='relu')(input_layer1)\n",
        "conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 5), activation='relu')(conv_layer1)\n",
        "conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(conv_layer2)\n",
        "print(conv_layer3.shape)\n",
        "conv3d_shape = conv_layer3.shape\n",
        "conv_layer3 = Reshape((conv3d_shape[1], conv3d_shape[2], conv3d_shape[3]*conv3d_shape[4]))(conv_layer3)\n",
        "conv_layer4 = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(conv_layer3)\n",
        "\n",
        "flatten_layer = Flatten()(conv_layer4)\n",
        "\n",
        "## fully connected layers\n",
        "dense_layer1 = Dense(units=256, activation='relu')(flatten_layer)\n",
        "dense_layer1 = Dropout(0.4)(dense_layer1)\n",
        "dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n",
        "dense_layer2 = Dropout(0.4)(dense_layer2)\n",
        "output_layer1 = Dense(units=output_units, activation='softmax')(dense_layer2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzFcDz_Hafkh"
      },
      "source": [
        "model_HSN = Model(inputs=input_layer1, outputs=output_layer1)\n",
        "model_HSN.summary()\n",
        "adam1 = Adam(lr=0.001, decay=1e-06)\n",
        "model_HSN.compile(loss='categorical_crossentropy', optimizer=adam1, metrics=['accuracy'])\n",
        "filepath = \"best_model_HSN.hdf5\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list1 = [checkpoint1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7186mesUcLOX",
        "collapsed": true
      },
      "source": [
        "model_HSN.fit_generator(TrainGen, steps_per_epoch=len(train_indices)//32, epochs=100, callbacks=callbacks_list1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-os21JEinngE"
      },
      "source": [
        "# function for creating a projected inception module\n",
        "def inception_module_2d(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n",
        "\t# 1x1 conv\n",
        "\tconv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n",
        "\t# 3x3 conv\n",
        "\tconv3 = Conv2D(f2_in, (1,1), padding='same', activation='relu')(layer_in)\n",
        "\tconv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu')(conv3)\n",
        "\t# 5x5 conv\n",
        "\tconv5 = Conv2D(f3_in, (1,1), padding='same', activation='relu')(layer_in)\n",
        "\tconv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu')(conv5)\n",
        "\t# 3x3 max pooling\n",
        "\tpool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
        "\tpool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(pool)\n",
        "\t# concatenate filters, assumes filters/channels last\n",
        "\tlayer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
        "\treturn layer_out\n",
        "\n",
        "\n",
        "# function for creating a projected inception module\n",
        "def inception_module_3d(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n",
        "\t# 1x1 conv\n",
        "\tconv1 = Conv3D(f1, (1,1,1), padding='same', activation='relu')(layer_in)\n",
        "\t# 3x3 conv\n",
        "\tconv3 = Conv3D(f2_in, (1,1,1), padding='same', activation='relu')(layer_in)\n",
        "\tconv3 = Conv3D(f2_out, (3,3,3), padding='same', activation='relu')(conv3)\n",
        "\t# 5x5 conv\n",
        "\tconv5 = Conv3D(f3_in, (1,1,1), padding='same', activation='relu')(layer_in)\n",
        "\tconv5 = Conv3D(f3_out, (5,5,5), padding='same', activation='relu')(conv5)\n",
        "\t# 3x3 max pooling\n",
        "\tpool = MaxPooling3D((3,3,3), strides=(1,1,1), padding='same')(layer_in)\n",
        "\tpool = Conv3D(f4_out, (1,1,1), padding='same', activation='relu')(pool)\n",
        "\t# concatenate filters, assumes filters/channels last\n",
        "\tlayer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
        "\treturn layer_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN602ymqn1Jx"
      },
      "source": [
        "# define model input\n",
        "visible = Input(shape=(25, 25, K, 1))\n",
        "\n",
        "# add 3d inception block 1\n",
        "layer = inception_module_3d(visible, 64, 96, 128, 16, 32, 32)\n",
        "layer = MaxPooling3D((3,3,3), strides=(2,2,2))(layer)\n",
        "# add 3d inception block 2\n",
        "#layer = inception_module_3d(layer, 128, 128, 192, 32, 96, 64)\n",
        "\n",
        "# Reshape for 2d conv\n",
        "conv3d_shape = layer.shape\n",
        "conv2d_layer = Reshape((conv3d_shape[1], conv3d_shape[2], conv3d_shape[3]*conv3d_shape[4]))(layer)\n",
        "\n",
        "# add 2d inception block 1\n",
        "conv2d_layer = inception_module_2d(conv2d_layer, 64, 96, 128, 16, 32, 32)\n",
        "conv2d_layer = MaxPooling2D((3,3), strides=(2,2))(conv2d_layer)\n",
        "\n",
        "flatten_layer = Flatten()(conv2d_layer)\n",
        "\n",
        "output_layer = Dense(units=output_units, activation='softmax')(flatten_layer)\n",
        "\n",
        "# create model\n",
        "model_I = Model(inputs=visible, outputs=output_layer)\n",
        "\n",
        "# summarize model\n",
        "model_I.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdA1F1Len4qr"
      },
      "source": [
        "# compiling the model\n",
        "adam2 = Adam(lr=0.001, decay=1e-06)\n",
        "model_I.compile(loss='categorical_crossentropy', optimizer=adam2, metrics=['accuracy'])\n",
        "\n",
        "# checkpoint\n",
        "filepath2 = \"best_model_I.hdf5\"\n",
        "checkpoint2 = ModelCheckpoint(filepath2, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list2 = [checkpoint2]\n",
        "\n",
        "model_I.fit_generator(TrainGen, steps_per_epoch=len(train_indices)//32, epochs=50, callbacks=callbacks_list2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2UHXJ-Lv97w"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TmSPySWcfmu"
      },
      "source": [
        "members = [model_IR,model_HSN,model_I]\n",
        "\n",
        "def ensemble_predictions(members, testX):\n",
        "\t# make predictions\n",
        "\tyhats = [model.predict_generator(testX) for model in members]\n",
        "\tyhats = np.array(yhats)\n",
        "\t# sum across ensemble members\n",
        "\tsummed = np.sum(yhats, axis=0)\n",
        "\t# argmax across classes\n",
        "\tresult = np.argmax(summed, axis=1)\n",
        "\treturn result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrAbCes-v97w"
      },
      "source": [
        "# load best weights\n",
        "model_IR.load_weights(\"best_model_IR.hdf5\")\n",
        "model_IR.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "model_HSN.load_weights(\"best_model_HSN.hdf5\")\n",
        "model_HSN.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "model_I.load_weights(\"best_model_I.hdf5\")\n",
        "model_I.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEq8is-Cv97x"
      },
      "source": [
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return each_acc, average_acc\n",
        "\n",
        "def reports (X_test,y_test,name):\n",
        "    #start = time.time()\n",
        "    Y_pred = ensemble_predictions(members,X_test)\n",
        "    # y_pred = np.argmax(Y_pred, axis=1)\n",
        "    #end = time.time()\n",
        "    #print(end - start)\n",
        "    if name == 'IP':\n",
        "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
        "                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
        "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
        "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
        "                        'Stone-Steel-Towers']\n",
        "    elif name == 'SA':\n",
        "        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n",
        "                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n",
        "                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n",
        "                        'Vinyard_untrained','Vinyard_vertical_trellis']\n",
        "    elif name == 'PU':\n",
        "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n",
        "                        'Self-Blocking Bricks','Shadows']\n",
        "    \n",
        "    classification = classification_report(y_test, Y_pred, target_names=target_names)\n",
        "    oa = accuracy_score(y_test, Y_pred)\n",
        "    confusion = confusion_matrix(y_test, Y_pred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(y_test, Y_pred)\n",
        "\n",
        "    # score = model.evaluate(X_test, batch_size=32)\n",
        "    # Test_Loss =  score[0]*100\n",
        "    # Test_accuracy = score[1]*100\n",
        "    \n",
        "    return classification, confusion, 0, 0, oa*100, each_acc*100, aa*100, kappa*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfpfy461v97x"
      },
      "source": [
        "classification, confusion, Test_loss, Test_accuracy, oa, each_acc, aa, kappa = reports(TestGen,np.subtract(test_labels,1),dataset)\n",
        "classification = str(classification)\n",
        "confusion = str(confusion)\n",
        "file_name = \"classification_report\" + \"_ensemble_\" + dataset + \"_\" + str(1-test_ratio) + \"ratio\" + \".txt\"\n",
        "\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write('{} Test loss (%)'.format(Test_loss))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Test accuracy (%)'.format(Test_accuracy))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Overall accuracy (%)'.format(oa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Average accuracy (%)'.format(aa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}'.format(classification))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}'.format(confusion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC-43MsmmoOp"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOyE0jMvmVSj"
      },
      "source": [
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return each_acc, average_acc\n",
        "\n",
        "def reports (X_test,y_test,name):\n",
        "    #start = time.time()\n",
        "    y_pred = model_HSN.predict_generator(X_test)\n",
        "    Y_pred = np.argmax(y_pred, axis=1)\n",
        "    #end = time.time()\n",
        "    #print(end - start)\n",
        "    if name == 'IP':\n",
        "        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
        "                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
        "                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
        "                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
        "                        'Stone-Steel-Towers']\n",
        "    elif name == 'SA':\n",
        "        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n",
        "                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n",
        "                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n",
        "                        'Vinyard_untrained','Vinyard_vertical_trellis']\n",
        "    elif name == 'PU':\n",
        "        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n",
        "                        'Self-Blocking Bricks','Shadows']\n",
        "    \n",
        "    classification = classification_report(y_test, Y_pred, target_names=target_names)\n",
        "    oa = accuracy_score(y_test, Y_pred)\n",
        "    confusion = confusion_matrix(y_test, Y_pred)\n",
        "    each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
        "    kappa = cohen_kappa_score(y_test, Y_pred)\n",
        "\n",
        "    # score = model.evaluate(X_test, batch_size=32)\n",
        "    # Test_Loss =  score[0]*100\n",
        "    # Test_accuracy = score[1]*100\n",
        "    \n",
        "    return classification, confusion, 0, 0, oa*100, each_acc*100, aa*100, kappa*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alh0Z_5BmcZg"
      },
      "source": [
        "classification, confusion, Test_loss, Test_accuracy, oa, each_acc, aa, kappa = reports(TestGen,np.subtract(test_labels,1),dataset)\n",
        "classification = str(classification)\n",
        "confusion = str(confusion)\n",
        "file_name = \"classification_report\" + \"_HSN_\" + dataset + \"_\" + str(1-test_ratio) + \"ratio\" + \".txt\"\n",
        "\n",
        "with open(file_name, 'w') as x_file:\n",
        "    x_file.write('{} Test loss (%)'.format(Test_loss))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Test accuracy (%)'.format(Test_accuracy))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Kappa accuracy (%)'.format(kappa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Overall accuracy (%)'.format(oa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{} Average accuracy (%)'.format(aa))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}'.format(classification))\n",
        "    x_file.write('\\n')\n",
        "    x_file.write('{}'.format(confusion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m80UbBdLYVhI"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU-y0IM7v97x"
      },
      "source": [
        "def Patch(data,height_index,width_index):\n",
        "    height_slice = slice(height_index, height_index+PATCH_SIZE)\n",
        "    width_slice = slice(width_index, width_index+PATCH_SIZE)\n",
        "    patch = data[height_slice, width_slice, :]\n",
        "    \n",
        "    return patch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PQdlwO-v97x"
      },
      "source": [
        "# load the original image\n",
        "X, y = loadData(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qid6g2eKv97x"
      },
      "source": [
        "height = y.shape[0]\n",
        "width = y.shape[1]\n",
        "PATCH_SIZE = windowSize\n",
        "numComponents = K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kr7ebNXv97x"
      },
      "source": [
        "X,pca = applyPCA(X, numComponents=numComponents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p3PsYBFv97x"
      },
      "source": [
        "X = padWithZeros(X, PATCH_SIZE//2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sbArQkvv97x"
      },
      "source": [
        "# calculate the predicted image\n",
        "outputs = np.zeros((height,width))\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        target = int(y[i,j])\n",
        "        if target == 0 :\n",
        "            continue\n",
        "        else :\n",
        "            image_patch=Patch(X,i,j)\n",
        "            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')                                   \n",
        "            prediction = (ensemble_predictions(members,X_test_image))\n",
        "            prediction = np.argmax(prediction, axis=1)\n",
        "            outputs[i][j] = prediction+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKEJgrc9v97x"
      },
      "source": [
        "ground_truth = spectral.imshow(classes = y,figsize =(7,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcV_PUaWv97x"
      },
      "source": [
        "predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(7,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qna1nsF3v97x"
      },
      "source": [
        "spectral.save_rgb(\"predictions.jpg\", outputs.astype(int), colors=spectral.spy_colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxOu6dvxnHnT"
      },
      "source": [
        "from google.colab import files\n",
        "files.dowhload('classification_report.txt')\n",
        "files.download('predictions.jpg') "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}